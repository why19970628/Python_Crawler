## Python_Crawler：Python爬虫和Python数据分析小项目

## 简介

可以用Python实现的小项目，内容包括Python爬虫、Python数据分析等，持续更新中。
  
本Repository主要用于存放项目代码，对应的项目文章可以关注CSDN博客。  

博客地址：https://blog.csdn.net/weixin_43746433

微信：why19970628

欢迎与我交流 ^_^

### 1.基本项目介绍
1. **Sina_Topic_Spider**:
- 内容： 爬取某位明星的微博超话的上万条用户信息，对爬取的结果进行EDA分析与数据可视化，如分析用户年龄，性别分布、粉丝团的地区分布，词云打榜微博内容。
- 对应CSDN文章：《[博客地址](https://blog.csdn.net/weixin_43746433)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★★☆☆


2. **LaGou**:
- 内容： 爬取拉勾网的职位的信息，爬取方式通过静态和动态网页，对爬取的结果进行EDA分析与数据可视化。
- 对应CSDN文章：《[Python爬虫实战之爬取拉勾网职位](https://blog.csdn.net/weixin_43746433)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★★☆☆



3. **ele_me**:
- 内容： 爬取饿了么某地区的外卖信息，并对外卖商铺信息、商品数据进行初步可视化。
- 对应CSDN文章：《[Python爬虫实战之爬取饿了么信息](https://blog.csdn.net/weixin_43746433/article/details/91906540)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★★☆☆



5. **DangDang_Books**:
- 内容：爬虫：当当网图书书名、书图、价格、简介、评分、评论数量等大约1000条Python图书数据。数据分析：图书评论数量分布的漏斗图、价格分布的柱状图、评论量Top、图书图片墙等可视化展示。
- 对应CSDN文章：《[当当网图书爬虫与数据分析](https://blog.csdn.net/weixin_43746433/article/details/91906540)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★☆☆☆



6. **LianJia**:
- 内容：多线程爬取链家的北京每个地区的所有小区的信息数据。
- 对应CSDN文章：《[爬取链家的小区信息](https://blog.csdn.net/weixin_43746433/article/details/95951341)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★☆☆☆



7. **51_job**:
- 内容： 爬取51job前程无忧简关于数据分析的职位信息，并对获取的数据进行数据清洗与分析，如各城市招聘岗位数、薪资与各城市工作地点数量，关系，学历，经验要求等关系、公司类型与对应岗位数、职位要求等可视化。
- 对应CSDN文章：《爬取51job前程无忧简历](https://blog.csdn.net/weixin_43746433/article/details/90490227)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★★☆☆



8. **Baidu_Music**:
- 内容： 批量下载百度音乐（千千音乐）任意歌手的所有歌曲。
- 对应CSDN文章：《[爬取百度音乐歌曲](https://blog.csdn.net/weixin_43746433/article/details/89814523)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★☆☆☆



9. **QiDian_Story**:
- 内容： 批量下载起点中文小说网的所有小说，自动生成对应小说文件夹，并获取某一文件夹下含有某字符结尾的文件信息。
- 对应CSDN文章：《[爬取起点小说](https://blog.csdn.net/weixin_43746433/article/details/91410332)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★☆☆☆



11. **DouBan_Movie**:
- 内容： 利用正则爬取豆瓣电影所有标签下的电影详情，数据导入数据库，并批量生成词云图。
- 对应CSDN文章：《[Python爬虫实战之爬取豆瓣详情以及影评](https://blog.csdn.net/weixin_43746433/article/details/90031364)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★☆☆☆


## 2.selenium框架爬虫项目介绍
1. **taobao**:
- 内容： 爬取淘宝搜索美食的所有页面，sql文件大约4500条数据，并对美食数据进行商品标题、销量排名与商铺信息、销量的城市排名、店铺所在城市分布情况、商品价格与销售额的关系等探索性数据分析。
- 对应CSDN文章：《[selenium爬取淘宝美食信息](https://blog.csdn.net/weixin_43746433/article/details/97623511)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★★☆☆



2. **Baidu_Address**:
- 内容： 利用selenium爬取百度地图的某地区的公司信息，包括公司名称，公司地址等。csv文件大约几十条数据
- 对应博客文章：《[博客地址](https://blog.csdn.net/weixin_43746433)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★☆☆☆
![Image text](https://github.com/why19970628/Python_Crawler/tree/master/Baidu_Address/image/smaple.PNG)



3. **DouYu**:
- 内容： 利用selenium爬取斗鱼网所有主播的类别，房间标题，房间ID，主播名称，热度，csv文件大约15000条数据。
- 对应CSDN文章：《[博客地址](https://blog.csdn.net/weixin_43746433)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★☆☆☆



4. **WangYi_Music**:
- 内容： 利用selenium爬取网易云音乐关于许嵩共计175首歌曲信息及歌词信息可视化。
- 对应CSDN文章：《[利用selenium爬取网易云音乐歌手歌曲信息并分析](https://blog.csdn.net/weixin_43746433/article/details/95243431)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★☆☆☆



## 3. Scrapy框架爬虫项目介绍

1. **Qsbk**:
- 内容： 利用Scrapy框架爬取糗事百科段子。
- 对应CSDN文章：《[利用Scrapy框架爬取糗事百科段子](https://blog.csdn.net/weixin_43746433/article/details/91636655)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★☆☆☆



2. **ChuanZhi_Class**:
- 内容： 利用Scrapy框架爬取传智播客课程数据。
- 对应CSDN文章：《[利用Scrapy框架爬取传智播客课程数据](https://blog.csdn.net/weixin_43746433/article/details/94732882)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★☆☆☆



3. **DangDang_Books/dangdang**:
- 内容：爬虫：利用Scrapy框架爬取当当网搜索界面图书书名、价格、评论数量等信息
- 对应CSDN文章：《[当当网图书爬虫与数据分析](https://blog.csdn.net/weixin_43746433/article/details/91906540)》
- 适合人群：Python爬虫学习者、Python数据分析学习者、Pandas使用者、数据可视化学习者
- 难度：★★☆☆☆



### 支持作者
熬夜敲代码写稿件，咖啡才是第一生产力。

<img src="https://github.com/why19970628/Python_Crawler/blob/master/coffee.png" width="300" />

喜欢作者的分享，如何支持作者？

Maybe you could buy me a cup of coffee. Salute!








